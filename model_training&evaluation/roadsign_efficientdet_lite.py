# -*- coding: utf-8 -*-
"""RoadSign:EfficientDet-Lite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SKUNCGkUPdTBRm_S7ptD8BpXR74bWBzh

# EfficientDet Lite for Road Sign Detection

**Training Data:**

Road Signs Dataset

URL: [https://makeml.app/datasets/road-signs](https://makeml.app/datasets/road-signs)

Make ML

Photo by Erik Mclean on Unsplash

<br>

**Road Sign Detection:**

This dataset contains 877 images of 4 different classes for road sign detection.

Classes:
* Traffic Light
* Stop
* Speed Limit
* Crosswalk

<br>

**Using Roboflow:**

The [data](https://www.kaggle.com/datasets/andrewmvd/road-sign-detection) is organized and converted using the [Roboflow](https://app.roboflow.com) platform.

[Using **Tflite Model Maker** with Google Colab](https://github.com/wwfish/tflite-model-maker-workaround/blob/main/README.md):
Google Colab has been updated to integrate Python 3.10, which causes the Tflite Model Maker API to malfunction (getting stuck in an infinite loop). The solution is to use a Conda environment with an older version of Python.
"""



# Commented out IPython magic to ensure Python compatibility.
# %env PYTHONPATH = # /env/python

!wget https://repo.anaconda.com/miniconda/Miniconda3-py39_23.3.1-0-Linux-x86_64.sh
!chmod +x Miniconda3-py39_23.3.1-0-Linux-x86_64.sh
!./Miniconda3-py39_23.3.1-0-Linux-x86_64.sh -b -f -p /usr/local
!conda update -q conda

import sys
sys.path.append('/usr/local/lib/python3.9/site-packages')

!conda create -n myenv python=3.9

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# eval "$(conda shell.bash hook)"
# conda activate myenv
# 
# !pip install roboflow
# 
# pip install tflite_runtime
# pip install tensorflow==2.9.1
# pip install tflite-model-maker==0.4.2
# pip install tflite-support==0.4.2
# 
# pip install pycocotools
# pip install opencv-python-headless==4.1.2.30
# pip install numpy==1.22.4
# pip install pandas==1.4.0
# 
# sudo apt -y install libportaudio2
# 
# pip install ipykernel

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# eval "$(conda shell.bash hook)"
# conda activate myenv
# pip install numpy==1.23.5

"""[Link to the dataset](https://app.roboflow.com/ds/6YNlJQywH3?key=e8yEvCoLv4) in VOC format and JPEG format (but the annotations and images are in the same folder)

[Link to the dataset (Kaggle)](https://www.kaggle.com/datasets/andrewmvd/road-sign-detection) in VOC format and PNG format (conversion needed)

In this case, the images are obtained from the Kaggle platform. An API key is required.

"""



from google.colab import files
import zipfile
import os

uploaded = files.upload() # kaggle.json

!pip install kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d andrewmvd/road-sign-detection

os.makedirs('/content/Road-Sign-3', exist_ok=True)

with zipfile.ZipFile("road-sign-detection.zip", "r") as zip_ref:
    zip_ref.extractall('/content/Road-Sign-3')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile train.py
# import os
# import xml.etree.ElementTree as ET
# from PIL import Image
# import random
# from tflite_model_maker.config import ExportFormat, QuantizationConfig
# from tflite_model_maker import model_spec
# from tflite_model_maker import object_detector
# from tflite_support import metadata
# import tensorflow as tf
# 
# 
# assert tf.__version__.startswith('2')
# 
# tf.get_logger().setLevel('ERROR')
# from absl import logging
# logging.set_verbosity(logging.ERROR)
# 
# print("\nTensorflow Version:")
# print(tf.__version__)
# print()
# 
# images_dir = '/content/Road-Sign-3/images'
# annotations_dir = '/content/Road-Sign-3/annotations'
# 
# def convert_images_to_jpeg_and_update_annotations(images_dir, annotations_dir):
#     for filename in os.listdir(images_dir):
#         if not filename.lower().endswith('.jpg') and not filename.lower().endswith('.jpeg'):
#             filepath = os.path.join(images_dir, filename)
#             new_filename = os.path.splitext(filename)[0] + '.jpg'
#             new_filepath = os.path.join(images_dir, new_filename)
# 
#             # Iz PNG v JPEG
#             with Image.open(filepath) as img:
#                 rgb_img = img.convert('RGB')
#                 rgb_img.save(new_filepath)
# 
#             annotation_file = os.path.join(annotations_dir, os.path.splitext(filename)[0] + '.xml')
#             if os.path.exists(annotation_file):
#                 tree = ET.parse(annotation_file)
#                 root = tree.getroot()
#                 root.find('filename').text = new_filename
#                 tree.write(annotation_file)
# 
#             os.remove(filepath)
# 
# convert_images_to_jpeg_and_update_annotations(images_dir, annotations_dir)
# 
# data = object_detector.DataLoader.from_pascal_voc(
#     images_dir,
#     annotations_dir,
#     label_map=['trafficlight', 'speedlimit', 'crosswalk', 'stop']
# )
# 
# # Split : 80-20
# def split_data(images_dir, annotations_dir, val_fraction=0.2):
#     images = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])
#     annotations = sorted([f for f in os.listdir(annotations_dir) if f.endswith('.xml')])
# 
#     assert len(images) == len(annotations), "The number of images and annotations must be the same"
# 
#     total_size = len(images)
#     val_size = int(total_size * val_fraction)
#     indices = list(range(total_size))
#     random.shuffle(indices)
#     val_indices = indices[:val_size]
#     train_indices = indices[val_size:]
# 
#     train_images = [images[i] for i in train_indices]
#     train_annotations = [annotations[i] for i in train_indices]
#     val_images = [images[i] for i in val_indices]
#     val_annotations = [annotations[i] for i in val_indices]
# 
#     train_data = object_detector.DataLoader.from_pascal_voc(
#         images_dir,
#         annotations_dir,
#         label_map=['trafficlight', 'speedlimit', 'crosswalk', 'stop'],
#         annotation_filenames=[os.path.splitext(f)[0] for f in train_annotations]
#     )
# 
#     val_data = object_detector.DataLoader.from_pascal_voc(
#         images_dir,
#         annotations_dir,
#         label_map=['trafficlight', 'speedlimit', 'crosswalk', 'stop'],
#         annotation_filenames=[os.path.splitext(f)[0] for f in val_annotations]
#     )
# 
#     return train_data, val_data
# 
# train_data, val_data = split_data(images_dir, annotations_dir)
# 
# spec = object_detector.EfficientDetLite1Spec()
# 
# # UÄenje
# model = object_detector.create(
#     train_data,
#     model_spec=spec,
#     batch_size=16,
#     train_whole_model=True,
#     epochs=8,
#     validation_data=val_data
# )
# 
# # Evaluacija (na validacijskih podatkih)
# eval_result = model.evaluate(val_data)
# print("********************************************************************************************************")
# print("Metrics:")
# for label, metric_value in eval_result.items():
#     print(f"{label}: {metric_value}")
# print()
# 
# # Izvoz
# model.export(export_dir='.', tflite_filename='road_sign_edl1.tflite')
# 
# 
# tflite_eval_result = model.evaluate_tflite('road_sign_edl1.tflite', val_data)
# print("********************************************************************************************************")
# print("TFLite Metrics:")
# for label, metric_value in tflite_eval_result.items():
#     print(f"{label}: {metric_value}")
#

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# eval "$(conda shell.bash hook)"
# conda activate myenv
# python /content/train.py
#

!ls

"""# Average Inference Time:"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile eval.py
# 
# import os
# import time
# import numpy as np
# import tensorflow as tf
# from PIL import Image
# from tflite_model_maker import object_detector
# from tqdm import tqdm
# import random
# 
# images_dir = '/content/Road-Sign-3/images'
# annotations_dir = '/content/Road-Sign-3/annotations'
# 
# def split_data(images_dir, annotations_dir, val_fraction=0.2):
#     images = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])
#     annotations = sorted([f for f in os.listdir(annotations_dir) if f.endswith('.xml')])
# 
#     assert len(images) == len(annotations), "The number of images and annotations must be the same"
# 
#     total_size = len(images)
#     val_size = int(total_size * val_fraction)
#     indices = list(range(total_size))
#     random.shuffle(indices)
#     val_indices = indices[:val_size]
#     train_indices = indices[val_size:]
# 
#     train_images = [images[i] for i in train_indices]
#     train_annotations = [annotations[i] for i in train_indices]
#     val_images = [images[i] for i in val_indices]
#     val_annotations = [annotations[i] for i in val_indices]
# 
#     return val_images, val_annotations
# 
# val_images, val_annotations = split_data(images_dir, annotations_dir)
# 
# def evaluate_tflite_model(tflite_model_path, val_images):
#     interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
#     interpreter.allocate_tensors()
# 
#     input_details = interpreter.get_input_details()
#     output_details = interpreter.get_output_details()
# 
#     input_shape = tuple(input_details[0]['shape'][1:3])  # Expected input shape (height, width)
# 
#     inference_times = []
# 
#     for idx, image in enumerate(tqdm(val_images, desc="Evaluating")):
#         img_path = os.path.join(images_dir, image)
# 
#         img = Image.open(img_path).resize(input_shape)  # Resize image to expected input size
#         input_tensor = np.expand_dims(np.array(img), axis=0).astype(np.uint8)  # Convert to UINT8
# 
#         interpreter.set_tensor(input_details[0]['index'], input_tensor)
# 
#         start_time = time.time()
#         interpreter.invoke()
#         end_time = time.time()
# 
#         inference_times.append(end_time - start_time)
# 
#     average_inference_time = np.mean(inference_times)
#     return average_inference_time
# 
# average_inference_time = evaluate_tflite_model('/content/road_sign_edl1.tflite', val_images)
# print("********************************************************************************************************")
# print(f"Average Inference Time: {average_inference_time} seconds")
#

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# eval "$(conda shell.bash hook)"
# conda activate myenv
# python /content/eval.py

"""# Example

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile example.py
# import numpy as np
# import tensorflow as tf
# from PIL import Image, ImageDraw, ImageFont
# 
# model_path = '/content/road_sign_edl1.tflite'
# interpreter = tf.lite.Interpreter(model_path=model_path)
# interpreter.allocate_tensors()
# 
# def preprocess_image(image_path, input_size):
#     img = Image.open(image_path).convert('RGB')
#     img_resized = img.resize(input_size)
#     img_array = np.array(img_resized)
#     img_array = img_array.astype('uint8')  # Change to uint8
#     img_array = np.expand_dims(img_array, axis=0)
#     return img, img_array
# 
# def draw_bounding_boxes(image, boxes, classes, scores, class_names, threshold=0.5):
#     draw = ImageDraw.Draw(image)
#     try:
#         font = ImageFont.truetype("arial.ttf", 20)
#     except IOError:
#         font = ImageFont.load_default()
# 
#     used_positions = []
# 
#     for i in range(len(boxes)):
#         if scores[i] >= threshold:
#             ymin, xmin, ymax, xmax = boxes[i]
#             xmin, ymin, xmax, ymax = int(xmin * image.width), int(ymin * image.height), int(xmax * image.width), int(ymax * image.height)
#             if xmin < 0: xmin = 0
#             if ymin < 0: ymin = 0
#             if xmax > image.width: xmax = image.width
#             if ymax > image.height: ymax = image.height
#             draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=3)
# 
#             text = f'{class_names[int(classes[i])]}: {scores[i]:.2f}'
#             text_bbox = draw.textbbox((xmin, ymin), text, font=font)
#             text_location = (xmin, ymin - (text_bbox[3] - text_bbox[1])) if ymin - (text_bbox[3] - text_bbox[1]) >= 0 else (xmin, ymin)
# 
#             while any([abs(text_location[1] - pos[1]) < (text_bbox[3] - text_bbox[1]) for pos in used_positions]):
#                 text_location = (text_location[0], text_location[1] - (text_bbox[3] - text_bbox[1] + 2))
#                 if text_location[1] < 0:
#                     text_location = (text_location[0], text_location[1] + (text_bbox[3] - text_bbox[1] + 2) * 2)
# 
#             used_positions.append(text_location)
#             text_background = (text_location[0] - 2, text_location[1] - 2, text_location[0] + (text_bbox[2] - text_bbox[0]) + 2, text_location[1] + (text_bbox[3] - text_bbox[1]) + 2)
# 
#             draw.rectangle(text_background, fill='white')
#             draw.text(text_location, text, fill='red', font=font)
# 
# image_path = '/content/Road-Sign-3/images/road238.jpg'
# input_size = (384, 384)  # Corrected input size
# original_image, preprocessed_image = preprocess_image(image_path, input_size)
# 
# input_details = interpreter.get_input_details()
# output_details = interpreter.get_output_details()
# print(f'Input details: {input_details}')
# print(f'Output details: {output_details}')
# print(f'Preprocessed image shape: {preprocessed_image.shape}')
# 
# interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
# 
# interpreter.invoke()
# 
# boxes = interpreter.get_tensor(output_details[1]['index'])[0]
# classes = interpreter.get_tensor(output_details[3]['index'])[0]
# scores = interpreter.get_tensor(output_details[0]['index'])[0]
# num_detections = int(interpreter.get_tensor(output_details[2]['index'])[0])
# 
# print(f'Boxes shape: {boxes.shape}')
# print(f'Boxes: {boxes}')
# print(f'Classes shape: {classes.shape}')
# print(f'Classes: {classes}')
# print(f'Scores shape: {scores.shape}')
# print(f'Scores: {scores}')
# print(f'Number of detections: {num_detections}')
# 
# valid_detections = scores[:num_detections] > 0.2
# print(f'the valid detection is {valid_detections}')
# boxes = boxes[:num_detections][valid_detections]
# print(f'the valid box is {boxes}')
# classes = classes[:num_detections][valid_detections]
# print(f'the valid class is {classes}')
# scores = scores[:num_detections][valid_detections]
# print(f'the valid score is {scores}')
# 
# class_names = ['trafficlight', 'speedlimit', 'crosswalk', 'stop']
# 
# draw_bounding_boxes(original_image, boxes, classes, scores, class_names)
# 
# output_image_path = 'annotated_road238.jpg'
# original_image.save(output_image_path)
# print(f"Annotated image saved as {output_image_path}")
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# eval "$(conda shell.bash hook)"
# conda activate myenv
# python /content/example.py
#

from PIL import Image
from IPython.display import display

annotated_image_path = 'annotated_road238.jpg'
annotated_image = Image.open(annotated_image_path)

bigger_image = annotated_image.resize((annotated_image.width * 2, annotated_image.height * 2))

display(bigger_image)